<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>KI - Künstliche Intelligenz und wie man sie braucht</title>
<link rel="stylesheet" href="assets/main.css">
<link rel="stylesheet" href="assets/chat.css">
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>
<body>
<!-- <header class="site-header" role="banner">

    <div class="wrapper"><a class="site-title" rel="author" href="/">KI - Künstliche Intelligenz und wie man sie braucht</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger">
            <label for="nav-trigger">
            <span class="menu-icon">
                <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
                </svg>
            </span>
            </label>
    
            <div class="trigger"><a class="page-link" href="/about/">About</a></div>
        </nav></div>
    </header> -->
    <main class="page-content" aria-label="Content">
        <div class="wrapper">
        <!-- 
    TODO 
    - --exkurs zu instruct training
    - --exkurs zu context limit
    - ende LLMs sind die Ultimativen Synonym sucher
    - DONT DO MATH WITH LLMS
-->
<chatn style="max-width: 80%; margin-left: auto; margin-right: auto; margin-bottom: 2em;"><span style="font-size:2em;">Was ist denn eigentlich ein "ChatGPT"?</span></chatn>

<p>Heute hat jeder natürlich von <a href="https://openai.com/index/chatgpt/">ChatGPT</a> gehört, und die meisten unter uns werden es auch schon mal verwendet haben. Allerdings verstehen die meisten Menschen nicht, wie diese Technologie tatsächlich funktioniert. Dies ist jedoch wenig überraschend, da diese Technologie für die meisten unerwartet aufgetaucht ist, das Feld sich so schnell entwickelt und sehr viel technischer Jargon involviert ist.<br />
Aber ChatGPT ist ein wertvolles Werkzeug und wie für jedes andere Werkzeug, sollte man verstehen, wie es tickt damit man es auch richtig anwenden kann.</p>

<p>Auf dieser kleinen Webseite werden die Grundprinzipien von ChatGPT und Co. einfach erklärt und anschaulich gemacht, um Ihnen zu helfen, diese neue Technologie möglichst effizient anzuwenden.</p>

<h1 id="eine-kurze-geschichte-der-sprachverarbeitung">Eine kurze Geschichte der Sprachverarbeitung</h1>
<p>“ChatGPT” ist in sogenanntes “Large Language Model” (Grosses sprach Model), oder kurz LLM. Man nennt sie “gross” aufgrund der unglaublichen Menge an Daten die verwendet werden, um sie zu trainieren.</p>

<p>Es mag überraschend klingen, aber LLMs sind nicht erst Ende 2022 aus dem Nichts entstanden. Tatsächlich sind sie die Frucht von langjähriger Entwicklungen im Feld der “Natural Language Processing” (Verarbeitung von natürlicher Sprache).<br /></p>

<p>Viele Menschen waren fasziniert von dem Konzept, dass man eines Tages vielleicht mit einem Computer sprechen könnte, und es gibt zahlreiche Beispiele von solchen intelligenten Maschinen in Filmen und Büchern. Aber auch Forscher träumten schon in den frühen Tagen der Informatik von solchen Maschinen.</p>

<p>So entwickelte man in den 60er Jahren (ca. 10 Jahre bevor es richtige Heimcomputer gab) schon ELIZA, ein extrem früher Chatbot, mit dem Ziel, Kommunikation zwischen Menschen zu erkunden. Durch das Erkennen einiger Schlüsselbegriffe, mit einigen vorprogrammierten Antworten, und der Fähigkeit, Abschnitte des menschlichen Inputs in die vorprogrammierten Antworten einzusetzen, konnte man tatsächlich eine sehr einfache Unterhaltung mit diesem Programm führen… Solange man nur bei ELIZAs “Regeln” spielte. Faszinierend dabei ist, dass schon damals einige Nutzer dachten, dass diese paar Linien Code ein menschliches Verstehen und Emotionen hätten. Ähnliche Meinungen kann man auch heute finden, wie beispielsweise <a href="https://www.nzz.ch/technologie/google-mitarbeiter-glaubt-dass-ein-ki-chatbot-bewusstsein-hat-ld.1688756">dieser Google Forscher</a>, der glaubte, dass ihr KI-Modell ein Bewusstsein hat.<br />
Dieses Zuschreiben eines Selbstbewusstseins zu etwas, was <em>nicht selbstbewusst sein kann</em> (Statistisches Modell, kein Konzept von Zeit, keine Denkprozesse, keine “Existenz” ausserhalb der Datenprozessierung) untermalt den unglaublichen Wert, den wir auf Kommunikation legen.</p>

<p>Viele solche einfache “Chatbots”, die essenziell nur grosse “wenn, dann” Maschinen waren, wurden über die Jahre gebaut, ohne dass je grössere Fortschritte gemacht wurden. Nur in den letzten 10-20 Jahren hatten wir endlich genügend Rechenkraft, um Neurale Netzwerk Methoden für Sprachen zu verwenden, eine Technologie, die nach den Neuronen Netzwerken in unserem Hirn modelliert ist. Wie dies im Detail funktioniert, wird an dieser Stelle nicht vertieft, jedoch ist es wichtig zu wissen, dass sie bei kürzeren Texten bereits eine gute Kohärenz zeigten. Jedoch, je länger und komplexer ein Text war, desto schwieriger wurde es für sie zu folgen.<br />
Auch wurden viele Schlüsseltechnologien zu dieser Zeit entdeckt, die heute noch in der GPT-Architektur verwendet werden, wie zum Beispiel “Word Embedding” (Wort Einbettung), welches später diskutiert wird.</p>

<p>Im Jahr 2017 gab es endlich einen Durchbruch in diesem Feld, der die vorhin erwähnten Kohärenzprobleme löste! Das revolutionäre “Attention is All You Need” (Aufmerksamkeit ist alles was man braucht) Paper von Vaswani et al. beschrieb eine KI Architektur auf der heute noch alle gängigen LLMs basieren; den <tooltip>"Transformer"<span>Das "GPT" von ChatGPT steht für "<b>G</b>enerative <b>P</b>re-Trained <b>T</b>ransformer".</span></tooltip>.</p>

<h1 id="und-wie-funktioniert-das">Und wie funktioniert das?</h1>
<p>Das “Attention is All You Need” Paper stellt das Transformer-Modell für das Verarbeiten und Generieren von Sprache vor. Mit dieser Methode wird jedem Wert in einer Sequenz (zum Beispiel in der Sprache jedes Wort in einem Satz) eine relative Wichtigkeit im Vergleich zu den anderen Werten in der Sequenz zugeordnet. Diese nennt man “Self-Attention” (Selbstaufmerksamkeit).</p>

<p>Um zu verstehen, was ein LLM aber genau mit diesen Werten macht, müssen wir uns ein komplexes Konzept anschauen: Nämlich, wie ein LLM überhaupt “versteht”.</p>

<h2 id="word-embedding-oder-der-konzept-raum">Word Embedding oder der “Konzept-Raum”</h2>
<p>Stellen wir uns einen Pfeil vor, der von links nach rechts zeigt. Dieser Pfeil repräsentiert ein Konzept oder eine Idee. Zum Beispiel steht das rechte Ende des Pfeils für “Tier”. Nun setzen wir Wörter, die mit dem Konzept “Tier” zu tun haben, an das rechte Ende des Pfeils und Wörter, die nichts mit Tieren zu tun haben, an das linke Ende.</p>

<p>Super, jetzt können wir zwischen Tieren und Nicht-Tieren unterscheiden, aber das reicht nicht, um einen Text wirklich zu verstehen, oder? Wie können wir unser Verständnis erweitern?</p>

<p>Stellen wir uns einen zweiten Pfeil vor, der diesmal für das Konzept “flauschig” steht. Wir haben uns hiermit von einem eindimensionalen “Vektor” zu einem zweidimensionalen “Raum” bewegt. Jetzt können wir zwischen flauschigen Tieren, flauschigen Nicht-Tieren, nicht-flauschigen Tieren und nicht-flauschigen Nicht-Tieren unterscheiden.</p>

<p>Wenn wir einen weiteren Pfeil hinzufügen, der für “Fleischfresser” steht, gelangen wir in einen dreidimensionalen Raum. Jetzt können wir Wörter noch genauer unterscheiden. Und vielleicht noch wichtiger, wir können Ähnlichkeiten zwischen Wörtern erkennen, die in ähnlichen Bereichen dieses Raums liegen.</p>

<div id="plotlyContainer">
    <div id="plotlyCheckboxContainer">
        <input type="checkbox" id="cbTier" onclick="handleDimension()" />
        <label for="cbTier">Konzept "Tier"</label>
        <input type="checkbox" id="cbFlausch" onclick="handleDimension()" />
        <label for="cbFlausch">Konzept "Flauschig"</label>
        <input type="checkbox" id="cbFleisch" onclick="handleDimension()" />
        <label for="cbFleisch">Konzept "Fleischfresser"</label>
    </div>
    <div id="plotlyChart"></div>
</div>
<p><br /></p>

<p>Moderne LLMs verwenden Tausende solcher “Konzept-Pfeile”. Je mehr dieser “Pfeile” wir hinzufügen, desto höherdimensional wird dieser “Konzept-Raum”, und desto besser können wir Wörter unterscheiden, vergleichen und verstehen. Der Fachbegriff für den “Konzept-Raum” ist “Word Embedding”: Die Wörter werden nach ihrer Bedeutung in einen hoch-dimensionalen Raum “eingebettet”. Und da wir über einen mathematischen Raum reden, nennt man die “Konzept-Pfeile” einfach “Vektor”.</p>

<h2 id="tokens-nicht-wörter">Tokens, nicht Wörter</h2>
<p>Streng genommen verstehen LLMs aber Text nicht als ganze Wörter, sondern als “Tokens” oder “Symbole”. Diese Tokens können ganze Wörter, Bruchteile von Wörtern, oder auch eines oder mehrere Zeichen sein. Wie ein Text genau in Tokens unterteilt wird, hängt vom Modell ab, aber immer muss das Modell einem Token eine “Bedeutung” zuweisen können, also in den oben besprochenen “Konzept-Raum” verorten können.<br />
So kann ein LLM statt jedes Wort einzeln zu lernen, eine viel kleinere Zahl von diesen Tokens verstehen und lernen, in welchen Anordnungen sie was heissen. Die Tokens sind also wie Buchstaben für das Modell.</p>

<p>Es mag verwundern, warum LLMs dann nicht einfach mit Buchstaben arbeiten. Das würde die Zahl Tokens extrem verringern. Hier hätten wir das umgekehrte Problem von “Ein Wort = Ein Token”, nämlich, dass “Ein Buchstabe = Ein Token” wieder zu ineffizient wäre. Wenn jedes Zeichen einzeln betrachtet würde, müsste das Modell extrem lange Sequenzen verarbeiten, um den Kontext eines Textes zu verstehen. Dies würde den Rechenaufwand erheblich erhöhen, weil längere Zeichenketten für jedes Wort nötig wären, um die Bedeutung zu rekonstruieren. Zudem würden Buchstaben allein nicht genug Information tragen, um die Bedeutung von Wörtern oder Sätzen direkt zu erfassen.</p>

<p>Ein Modell, das nur mit einzelnen Buchstaben arbeitet, müsste viel mehr Zeit damit verbringen, Muster und Zusammenhänge zu lernen, da es keine vorgefertigten Einheiten wie Wortteile oder häufige Ausdrücke nutzen könnte.</p>

<style>
.tokenizer-tkn {
    display:inline-block;
    white-space:pre;
    position: relative;
    font-family: monospace;
}
.tokenizer-tkn-0 {
    background:rgba(107,64,216,.3)
}
.tokenizer-tkn-1 {
    background:rgba(104,222,122,.4)
}
.tokenizer-tkn-2 {
    background:rgba(244,172,54,.4)
}
.tokenizer-tkn-3 {
    background:rgba(239,65,70,.4)
}
.tokenizer-tkn-4 {
    background:rgba(39,181,234,.4)
}

.tokenizer-output #ids {
    color:#00000000;
    /* display: flex; */
    flex-flow: wrap;
}

#ids .tokenizer-tkn:before {
    color: black;
    position: absolute;
    font-size: 0.5em;
    top: 30%;
    user-select: text;
}
#ids span {
    position: relative;
    display: flex;
    justify-content: center;
    align-items: center;
    width: fit-content;
    flex-direction: row;
    user-select: none;
}

#ids .tn0:before {
    content: "51486";
}
#ids .tn1:before {
    content: "11";
}
#ids .tn2:before {
    content: "71286";
}
#ids .tn3:before {
    content: "28654";
}
#ids .tn4:before {
    content: "89";
}
#ids .tn5:before {
    content: "11";
}
#ids .tn6:before {
    content: "813";
}
#ids .tn7:before {
    content: "5547";
}
#ids .tn8:before {
    content: "844";
}
#ids .tn9:before {
    content: "5547";
}
#ids .tn10:before {
    content: "13256";
}
#ids .tn11:before {
    content: "4629";
}
#ids .tn12:before {
    content: "58683";
}
#ids .tn13:before {
    content: "115414";
}
#ids .tn14:before {
    content: "36745";
}
#ids .tn15:before {
    content: "1180";
}
#ids .tn16:before {
    content: "10762";
}
#ids .tn17:before {
    content: "26152";
}
#ids .tn18:before {
    content: "29577";
}
#ids .tn19:before {
    content: "163518";
}
#ids .tn20:before {
    content: "13";
}
#ids .tn21:before {
    content: "100260";
}
#ids .tn22:before {
    content: "13383";
}
#ids .tn23:before {
    content: "26219";
}
#ids .tn24:before {
    content: "1462";
}
#ids .tn25:before {
    content: "11";
}
#ids .tn26:before {
    content: "1713";
}
#ids .tn27:before {
    content: "16637";
}
#ids .tn28:before {
    content: "2434";
}
#ids .tn29:before {
    content: "50554";
}
#ids .tn30:before {
    content: "26826";
}
#ids .tn31:before {
    content: "9080";
}
#ids .tn32:before {
    content: "11";
}
#ids .tn33:before {
    content: "30401";
}
#ids .tn34:before {
    content: "82183";
}
#ids .tn35:before {
    content: "146912";
}
#ids .tn36:before {
    content: "3075";
}
#ids .tn37:before {
    content: "1076";
}
#ids .tn38:before {
    content: "86113";
}
#ids .tn39:before {
    content: "39310";
}
#ids .tn40:before {
    content: "1227";
}
#ids .tn41:before {
    content: "97134";
}
#ids .tn42:before {
    content: "12";
}
#ids .tn43:before {
    content: "16981";
}
#ids .tn44:before {
    content: "180764";
}
#ids .tn45:before {
    content: "13";
}

.tokenizer-tkn {
    font-size: 1.3em;
}

</style>

<div class="tokenizer-output">
    <div id="tokenizer-view-switch" class="switch-field switch-orientation-horizontal switch-theme-default visible">
        <input checked="" type="radio" id="tokenizer-view-switch-radio-text" name="tokenizer-view-switch-switch" value="text" /><label for="tokenizer-view-switch-radio-text">Text</label>
        <input type="radio" id="tokenizer-view-switch-radio-bpe" name="tokenizer-view-switch-switch" value="bpe" /><label for="tokenizer-view-switch-radio-bpe">Token IDs</label>
    </div>
    <div id="text">
        <span><span class="tokenizer-tkn tokenizer-tkn-0 tn0">Ach</span><span class="tokenizer-tkn tokenizer-tkn-1 tn1">,</span></span><span class="tokenizer-tkn tokenizer-tkn-2 tn2"> kleiner</span><span class="tokenizer-tkn tokenizer-tkn-3 tn3"> Prin</span><span class="tokenizer-tkn tokenizer-tkn-4 tn4">z</span><span class="tokenizer-tkn tokenizer-tkn-0 tn5">,</span><span class="tokenizer-tkn tokenizer-tkn-1 tn6"> so</span><span class="tokenizer-tkn tokenizer-tkn-2 tn7"> nach</span><span class="tokenizer-tkn tokenizer-tkn-3 tn8"> und</span><span class="tokenizer-tkn tokenizer-tkn-4 tn9"> nach</span><span class="tokenizer-tkn tokenizer-tkn-0 tn10"> habe</span><span class="tokenizer-tkn tokenizer-tkn-1 tn11"> ich</span><span class="tokenizer-tkn tokenizer-tkn-2 tn12"> dein</span><span class="tokenizer-tkn tokenizer-tkn-3 tn13"> kleines</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn14"> schw</span><span class="tokenizer-tkn tokenizer-tkn-0 tn15">erm</span><span class="tokenizer-tkn tokenizer-tkn-1 tn16">üt</span><span class="tokenizer-tkn tokenizer-tkn-2 tn17">iges</span></span><span class="tokenizer-tkn tokenizer-tkn-3 tn18"> Leben</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn19"> verstanden</span><span class="tokenizer-tkn tokenizer-tkn-0 tn20">.</span></span><span class="tokenizer-tkn tokenizer-tkn-1 tn21"> Lange</span><span class="tokenizer-tkn tokenizer-tkn-2 tn22"> Zeit</span><span class="tokenizer-tkn tokenizer-tkn-3 tn23"> hast</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn24"> du</span><span class="tokenizer-tkn tokenizer-tkn-0 tn25">,</span></span><span class="tokenizer-tkn tokenizer-tkn-1 tn26"> um</span><span class="tokenizer-tkn tokenizer-tkn-2 tn27"> dich</span><span class="tokenizer-tkn tokenizer-tkn-3 tn28"> zu</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn29"> zer</span><span class="tokenizer-tkn tokenizer-tkn-0 tn30">stre</span><span class="tokenizer-tkn tokenizer-tkn-1 tn31">uen</span><span class="tokenizer-tkn tokenizer-tkn-2 tn32">,</span></span><span class="tokenizer-tkn tokenizer-tkn-3 tn33"> nichts</span><span class="tokenizer-tkn tokenizer-tkn-4 tn34"> anderes</span><span class="tokenizer-tkn tokenizer-tkn-0 tn35"> gehabt</span><span class="tokenizer-tkn tokenizer-tkn-1 tn36"> als</span><span class="tokenizer-tkn tokenizer-tkn-2 tn37"> die</span><span><span class="tokenizer-tkn tokenizer-tkn-3 tn38"> Lieb</span><span class="tokenizer-tkn tokenizer-tkn-4 tn39">lichkeit</span></span><span class="tokenizer-tkn tokenizer-tkn-0 tn40"> der</span><span><span class="tokenizer-tkn tokenizer-tkn-1 tn41"> Sonnen</span><span class="tokenizer-tkn tokenizer-tkn-2 tn42">-</span></span><span><span class="tokenizer-tkn tokenizer-tkn-3 tn43">unter</span><span class="tokenizer-tkn tokenizer-tkn-4 tn44">gänge</span><span class="tokenizer-tkn tokenizer-tkn-0 tn45">.</span></span>
    </div>
    <div id="ids">
        <span><span class="tokenizer-tkn tokenizer-tkn-0 tn0">Ach</span><span class="tokenizer-tkn tokenizer-tkn-1 tn1">,</span></span><span class="tokenizer-tkn tokenizer-tkn-2 tn2"> kleiner</span><span class="tokenizer-tkn tokenizer-tkn-3 tn3"> Prin</span><span class="tokenizer-tkn tokenizer-tkn-4 tn4">z</span><span class="tokenizer-tkn tokenizer-tkn-0 tn5">,</span><span class="tokenizer-tkn tokenizer-tkn-1 tn6"> so</span><span class="tokenizer-tkn tokenizer-tkn-2 tn7"> nach</span><span class="tokenizer-tkn tokenizer-tkn-3 tn8"> und</span><span class="tokenizer-tkn tokenizer-tkn-4 tn9"> nach</span><span class="tokenizer-tkn tokenizer-tkn-0 tn10"> habe</span><span class="tokenizer-tkn tokenizer-tkn-1 tn11"> ich</span><span class="tokenizer-tkn tokenizer-tkn-2 tn12"> dein</span><span class="tokenizer-tkn tokenizer-tkn-3 tn13"> kleines</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn14"> schw</span><span class="tokenizer-tkn tokenizer-tkn-0 tn15">erm</span><span class="tokenizer-tkn tokenizer-tkn-1 tn16">üt</span><span class="tokenizer-tkn tokenizer-tkn-2 tn17">iges</span></span><span class="tokenizer-tkn tokenizer-tkn-3 tn18"> Leben</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn19"> verstanden</span><span class="tokenizer-tkn tokenizer-tkn-0 tn20">.</span></span><span class="tokenizer-tkn tokenizer-tkn-1 tn21"> Lange</span><span class="tokenizer-tkn tokenizer-tkn-2 tn22"> Zeit</span><span class="tokenizer-tkn tokenizer-tkn-3 tn23"> hast</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn24"> du</span><span class="tokenizer-tkn tokenizer-tkn-0 tn25">,</span></span><span class="tokenizer-tkn tokenizer-tkn-1 tn26"> um</span><span class="tokenizer-tkn tokenizer-tkn-2 tn27"> dich</span><span class="tokenizer-tkn tokenizer-tkn-3 tn28"> zu</span><span><span class="tokenizer-tkn tokenizer-tkn-4 tn29"> zer</span><span class="tokenizer-tkn tokenizer-tkn-0 tn30">stre</span><span class="tokenizer-tkn tokenizer-tkn-1 tn31">uen</span><span class="tokenizer-tkn tokenizer-tkn-2 tn32">,</span></span><span class="tokenizer-tkn tokenizer-tkn-3 tn33"> nichts</span><span class="tokenizer-tkn tokenizer-tkn-4 tn34"> anderes</span><span class="tokenizer-tkn tokenizer-tkn-0 tn35"> gehabt</span><span class="tokenizer-tkn tokenizer-tkn-1 tn36"> als</span><span class="tokenizer-tkn tokenizer-tkn-2 tn37"> die</span><span><span class="tokenizer-tkn tokenizer-tkn-3 tn38"> Lieb</span><span class="tokenizer-tkn tokenizer-tkn-4 tn39">lichkeit</span></span><span class="tokenizer-tkn tokenizer-tkn-0 tn40"> der</span><span><span class="tokenizer-tkn tokenizer-tkn-1 tn41"> Sonnen</span><span class="tokenizer-tkn tokenizer-tkn-2 tn42">-</span></span><span><span class="tokenizer-tkn tokenizer-tkn-3 tn43">unter</span><span class="tokenizer-tkn tokenizer-tkn-4 tn44">gänge</span><span class="tokenizer-tkn tokenizer-tkn-0 tn45">.</span></span>
    </div>
    <span style="font-size:0.75em; margin-left:1em;">Antoine de Saint-Exupéry. <i>Der Kleine Prinz</i>. Übersetzt von Grete und Josef Leitgeb, Karl Rauch Verlag, 1956. <a href="https://platform.openai.com/tokenizer" target="_self">GPT-4o Tokenizer</a>.</span>
    <div class="info" id="text">
        Die Tokens sind hier mit verschiedenen Farben gekennzeichnet. Viele Wörter sind tatsächlich ein einzelnes Token, jedoch gibt es auch Wörter wie "unter|gänge", die klar in für uns sinnvolle Bruchstücke zerlegt wurden. Die Unterteilung von "zer|stre|uen" ist für uns wiederum schwieriger zu rationalisieren, aber offenbar kommen diese Wortfragmente oft genug vor (verteilt über alle trainierten Sprachen), dass es statistisch sinnvoll war, sie zu verwenden.<br />
        Noch strenger genommen, ist dies aber immer noch nicht wie ein LLM Text "liest". Klicken Sie auf den "Token IDs" Knopf über dem Text.
    </div>
    <div class="info" id="ids" style="color:initial;">
        Jedem dieser Tokens wurde eine ID-Nummer zugewiesen, und das ist, mit was ein LLM in Wirklichkeit arbeitet. Es hat verstanden, dass das Token 13 (ein ".") am Ende eines Satzes stehen muss, dass das Token 86113, gefolgt von Token 39310 das Wort "Lieblichkeit" ist etc.<br />
        Ein Text wird in diese Token IDs umgewandelt, bevor es an das LLM gesendet wird, und das Modell antwortet in Token IDs, welche wieder zu Text konvertiert werden müssen, damit wir es wieder lesen können.<br />
        Tokens sind auch der Grund, weshalb es gelegentlich schwer ist für ein LLM Wörter zu buchstabieren, da für sie die kleinste Einheit eines Wortes eigentlich ein Token ist. Das Buchstabieren muss separat gelernt werden.
    </div>

</div>
<p><br /></p>

<p>Neuere Modelle tendieren jedoch mehr und mehr auf ein “Ein Wort = Ein Token” Schema. Es ist möglich, dass dies in naher Zukunft der Standard wird.</p>

<h2 id="die-vorhersage">Die Vorhersage</h2>
<p>Aber wie hilft der Konzept-Raum dem LLM, zu entscheiden, welches Wort (oder welches Token) es als Nächstes vorhersagen muss? Hier kommt die Selbstaufmerksamkeit ins Spiel. Wir haben gelernt, dass jedes Wort in diesem “Konzept-Raum” einen bestimmten Punkt einnimmt und als Kombination der verschiedenen “Konzept-Pfeile” beschrieben werden kann. Es gibt aber Wörter mit mehreren Bedeutungen, wie zum Beispiel “Schloss”. Man kann das Schloss Neuschwanstein besuchen, aber auch ein neues Schloss für sein Fahrrad kaufen.</p>

<p>An diesem Punkt nutzt das LLM die Aufmerksamkeit. Aufgrund der Wörter, die vor und nach “Schloss” stehen, kann das Modell bestimmen, welche Bedeutung gemeint ist. Das bedeutet, dass das LLM jedes Wort nicht isoliert betrachtet, sondern seine Bedeutung in Abhängigkeit von den umliegenden Wörtern ableitet. Genau das macht die Transformer-Architektur so mächtig.</p>

<p>Wenn Sie bereits ein LLM verwendet haben, ist Ihnen möglicherweise aufgefallen, dass der Text Wort für Wort generiert wird. Nun verstehen wir, warum das so ist: <br />
Weil nach jedem Wort die Selbstaufmerksamkeit neu berechnet werden muss, damit das Modell verstehen kann, in welchem Kontext das nächste Wort steht.
Aber wie entscheidet das Modell, welches dieses nächste Wort ist? Dieser Vorgang wird Vorhersage oder “Prediction” genannt.</p>

<p>Stellen Sie sich vor, wir haben einen Satz, der gerade noch unvollständig ist, zum Beispiel: “Der Hund ist sehr …”. Das Modell erkennt aufgrund der vorangegangenen Wörter, dass wir uns wahrscheinlich noch im Konzept-Raum von ‘Tier’ befinden. Aber welches Wort könnte als Nächstes kommen? Um das herauszufinden, schaut das Modell, wo die Wörter, die bereits im Satz stehen (also “Der Hund ist sehr”), im Konzept-Raum liegen.</p>

<p>Nun versucht das LLM, das wahrscheinlichste nächste Wort zu finden, indem es nach Wörtern sucht, die sich in der Nähe dieser schon vorhandenen Wörter im Konzept-Raum befinden. Wenn der Satz zum Beispiel mit “Der Hund ist sehr …” beginnt, könnte das Modell feststellen, dass nach “sehr” meistens ein Adjektiv folgt und da dieses Adjektiv einen Hund beschreibt, geht es um ein Haustier. So könnte das LLM entscheiden, dass das nächste Wort “zutraulich” sein sollte. Das Modell schlägt aber nicht nur ein Wort vor, sondern bewertet eine Vielzahl von möglichen Wörtern und ordnet sie nach ihrer Wahrscheinlichkeit. Versuchen Sie hier über “zutraulich” zu hovern:</p>

<chatb>[...] Dieser Hund ist sehr <span class="variant">zutraulich<div class="alternatives">
    <div>zutraulich<br />24.2%</div>
    <div>schüchtern<br />22.5%</div>
    <div>klug<br />10.3%</div>
    <div>&nbsp;&nbsp;[...]&nbsp;&nbsp;</div>
    <div>ist<br />0.04%</div>
    <div>Ente<br />0.01%</div>
</div>
</span> <div class="thinking">
    <span class="dot"></span>
    <span class="dot"></span>
    <span class="dot"></span>
</div></chatb>

<p>So funktioniert der Vorhersageprozess in einem LLM: Es wählt das nächste Wort auf Basis der bereits bekannten Wörter und ihrer Positionen im Konzept-Raum aus. Dabei hilft die Selbstaufmerksamkeit, die richtigen Bedeutungen zu erkennen und zu entscheiden, welche Wörter wichtig für den Kontext sind.</p>

<div class="exkursContainer">
<input class="exkurs" type="checkbox" id="exkursKontext" />
<label class="exkurs" for="exkursKontext"><span>Warum vergessen LLMs Dinge so schnell?</span></label>
    <exkursText>
    <chatb>Aufgrund <b>Hardware- und Trainingslimitationen</b>, kann ein LLM nicht unendlich viel Aufmerksamkeit berechnen. Deswegen können LLMs immer nur den letzen Abschnitt des Textes anschauen um das nächste Wort zu bestimmen. Dies nennt man das "Context Window", das <b>Kontextfenster</b> des Modells. Wie jeder Text der von LLMs verarbeitet wird, wird auch das Kontextfenster in Tokens gemessen. Ende 2024 haben die meisten Modelle ein Kontextfenster von mindestens ~8'000 Tokens. Je nachdem wie der Text tokenisiert wird, sind das zwischen 10 und 13 A4 Arial 12pt Seiten Text. An alles was ausserhalb dieses Kontextfensters liegt, kann sich das Modell nicht mehr "erinnern" und es kann also die Wahl des nächsten Wortes nicht mehr beeinflussen.<br />
    GPT-4 Turbo, eine bezahlte Version von ChatGPT, hat ein Kontextfenster von ganzen 128'000 Tokens, was ungefähr 285 Seiten entspricht. Die Kontextfenster der Gratisversionen sind zum Zeitpunkt der Verfassung dieses Textes nicht offiziell bekannt.</chatb>
    </exkursText>
</div>

<h2 id="ja-aber-von-wo-kommt-dieser-konzept-raum">Ja, aber von wo kommt dieser “Konzept-Raum”?</h2>
<p>Vielleicht haben Sie sich schon gewundert, wie ein LLM überhaupt weiss, wo ein Wort sich im Konzept-Raum befindet. Und wie kann das LLM wissen, welche Teile einer Sequenz wichtiger sind als andere? Die Antwort darauf findet sich im Trainingsprozess des Modells, der auf einem umfassenden Versuch-und-Irrtum-Verfahren beruht.</p>

<p>Anfangs versteht das Modell gar nichts. Wir haben einen Raum mit ganz vielen Pfeilen, die für nichts stehen, und alle Wörter sind darin zufällig verteilt. Zusätzlich haben wir Milliarden von sogenannten “Weights” (Gewichtungen), die verwendet werden, um die Selbstaufmerksamkeit zu berechnen. Auch diese sind zuerst zufällige Zahlen.</p>

<p>Nun probiert das LLM, Text zu generieren, und dieser wird mit dem Text im Trainingsmaterial verglichen – Millionen von Seiten menschlich geschriebenen Textes. Anfangs macht das Modell viele Fehler, weil die Wörter noch zufällig im Konzept-Raum verteilt sind und die Gewichtungen noch nicht richtig eingestellt sind. Der generierte Text ist völlig unverständlich.</p>

<p>Hier beginnt der Prozess des Versuchs und Irrtums. Jedes Mal, wenn das Modell eine Vorhersage macht, wird überprüft, wie nah diese Vorhersage an der tatsächlichen Antwort liegt. Dann passt es die Gewichtungen an, bewegt die Positionen der Wörter im Konzept-Raum und probiert es nochmals. Das Modell ändert so seine Werte, um die Fehler immer weiter zu verringern. Im Grunde “lernt” das Modell durch seine eigenen Fehler und generiert so komplett selbstständig ein Verständnis der Sprache.</p>

<p>Hier einige Beispiele eines Modells an verschiedenen Trainings-Stufen:</p>

<form class="chatVariantButtons">
    <input type="radio" id="early" name="selection" value="early" checked="" />
    <label for="early">Früh</label>
    <input type="radio" id="mid" name="selection" value="mid" />
    <label for="mid">Mitte</label>
    <input type="radio" id="late" name="selection" value="late" />
    <label for="late">Spät</label>
</form>

<chatn>Was ist ein Hund?</chatn>
<chatvariant>
<chatb id="early">Sell enthochled fast diese rrrönographOefeennt-text was sus jurored tatymoyeen <div class="thinking">
    <span class="dot"></span>
    <span class="dot"></span>
    <span class="dot"></span>
</div></chatb>
<div class="info" id="early">
    Hier ist das Modell sehr früh im Training. Trotzdem sehen wir, dass es schon einigermassen verstanden hat, dass es auf Deutsch antworten sollte. Es scheint also, dass "Sprache" ein Konzept ist, welches sehr früh gelernt wird.
</div>
<chatb id="mid">Ein Hund (Canis lupus familiaris) ist die zweibeinige Haushaltungsspecies des Echte Wolfes (Canis lupus). <div class="thinking">
    <span class="dot"></span>
    <span class="dot"></span>
    <span class="dot"></span>
</div></chatb>
<div class="info" id="mid">
    Das Modell ist noch mitten im Training. Der Text ist deutlich kohärenter als vorher, und komplett auf Deutsch, also hat es schon sehr gut gelernt, Text einzuordnen. Jedoch beinhaltet die Antwort viele grammatische und fachliche Fehler, was darauf hinweist, dass das Modell immer noch viel zu lernen hat.
</div>
<chatb id="late">Ein Hund ist ein domestiziertes Säugetier und gehört zur Familie der Canidae (Hundeartigen). <div class="thinking">
    <span class="dot"></span>
    <span class="dot"></span>
    <span class="dot"></span>
</div></chatb>
<div class="info" id="late">
    Dies ist die Antwort eines vollständig trainierten LLMs. Der Text ist komplett kohärent, und fachlich korrekt, da es genügend gut "gelernt" hat, was ein Hund ist und wie ein Satz aussehen sollte.
</div>
</chatvariant>
<p><br /></p>

<p>Am Ende dieses intensiven Trainingsprozesses hat das LLM schlussendlich gelernt, die Bedeutung und Beziehung von Wörtern auf eine Art und Weise zu erkennen, die es ihm erlaubt, natürliche Sprache zu “verstehen” und zu generieren.</p>

<p>Aber es ist wichtig für uns zu realisieren, dass <strong>ein LLM im Grunde genommen nichts versteht!</strong>
Es “weiss” nicht, dass Katzen und Hunde beide Tiere sind, aber weil sie im Trainingsmaterial oft in einem ähnlichen Kontext vorkommen, werden diese beiden Wörter im Kontextraum nahe beieinander liegen.<br />
Durch einen Prozess von ständigem Trial-and-Error wird das Modell zunehmend besser darin, den Zusammenhang zwischen Wörtern und ihren Bedeutungen zu erfassen, doch <strong>das Verständnis bleibt rein statistisch und basiert auf Mustern, die im Trainingsmaterial gefunden wurden</strong> – nicht auf echtem Wissen oder Verständnis.</p>

<div class="exkursContainer">
<input class="exkurs" type="checkbox" id="exkursChat" />
<label class="exkurs" for="exkursChat"><span>Wie kommen wir von Textvervollständigung to Chat?</span></label>
<exkursText>
<chatb>Bis jetzt wurde immer nur von der "Vorhersage" von Wörtern gesprochen, also eigentlich einer Textvervollständigungs-Methode, nicht viel anders als die Textvorschläge, die Sie oben an Ihrer Smartphonetastatur bekommen.<br />
Wenn man eines dieser Vervollständigungs Modelle fragt

<chatn>Was ist ein Hund?</chatn>
würde es, statt dies als Frage zu interpretieren, es als Teil eines Textes ansehen. So wird es also, anstatt auf die Frage zu antworten, den Text einfach weiterführen, was so aussehen könnte:

<chatb>Ein Tier mit vier Beinen. Was ist ein Baum? Eine dicke Pflanze. Was ist ein Huhn? [...]</chatb>

Wie Sie sehen können, ist dies nicht besonders hilfreich. Aber <b>es ist eine klare Tendenz zur Nachahmung der Struktur des ursprünglichen Textes sichtbar</b>. Diese Beobachtung haben Nutzer und Forscher auch schon früh gemacht und haben versucht, es sich zu Nutze zu machen:

Gibt man einem LLM eine Seite eines Transkripts eines Interviews oder Ähnliches, wird es probieren, dieses weiterzuführen:

<chatn>Nutzer: [...]
Assistent: [...]
Nutzer:</chatn> 
<chatb>[...]
Assistent: [...]
Nutzer: [...]
Assistent: <i>etc.</i></chatb>

Ein kleines Problem mit dieser Methode ist jedoch, dass das LLM auch den nächsten Text vom Nutzer vorhersagen wird. Aber <b>offensichtlich, sind diese LLMs schon so in der Lage "Unterhaltungen" zu führen. Es gilt also eigentlich nur, das Modell "hilfreicher" zu machen</b>, und es daran zu hindern, für den Nutzer zu antworten.

Dies wird mit sogenanntem "Instruct Finetuning" gemacht. Nach dem eigentlichen Training, wird das Modell nochmals trainiert. Diesmal aber nicht zum Erkennen von Verhältnissen zwischen Wörtern, sondern um ein bestimmtes Format zu lernen: Sogenannte "Instruct Templates" (Instruktions-Vorlagen).<br />
Im einfachsten Fall, kann das etwas Ähnliches sein, wie das "Nutzer: [...],  Assistent: [...]" Beispiel. Gleichzeitig wird auch die Software, welches das Modell kontrolliert, so modifiziert, dass wen das Modell versucht "Nutzer:" zu generieren, es stattdessen gestoppt wird, damit der echte Nutzer antworten kann.

Und das ist schon die gesamte Magie hinter diesen sogenannten "Instruct" Modellen, mit denen wir Chatten können: <b>Das Modell wird nach dem Basistraining nochmals trainiert, um es dazu bringen in einem gewissen Format zu antworten</b> und Software wandelt dies visuell in einen "Chat" um.</chatb>
</exkursText>
</div>

<h1 id="take-aways">Take-Aways</h1>

<p>Es gibt drei Punkte, welche Sie bei der Benutzung eines LLMs immer im Hinterkopf behalten sollten:</p>

<ul>
  <li>
    <p><strong>LLMs sind NICHT intelligent!</strong><br />
Ein LLM schreibt Text, der zusammenhängend <em>aussieht</em>, basierend auf dem Trainingsmaterial UND Ihrem Input!<br /></p>
  </li>
  <li>
    <p><strong>LLMs lügen gut und gerne!</strong><br />
Essenziell hat ein LLM nur gelernt, wie man überzeugend aussehenden Text schreibt. <u>Dass sie relativ oft die Wahrheit schreiben, ist eigentlich nur ein Nebeneffekt</u>.<br />
Mit sehr viel Ach und Krach hat man geschafft, sie dazu zu bringen, <em>manchmal</em> zuzugeben, wenn sie etwas nicht wissen. Trotzdem schreiben sie oft etwas, das aussieht wie eine richtige Antwort, aber in Wirklichkeit Kreuz falsch ist. Dies nennt man eine “Halluzination”.<br />
Ein unterhaltsames Beispiel hiervon finden Sie in <a href="https://www.n-tv.de/panorama/Anwalt-vertraute-zu-sehr-auf-ChatGPT-article24152429.html">diesem Artikel</a>, wo ein Anwalt für seine Anklagenschrift ChatGPT verwendete, und es Fälle frei erfand.</p>
  </li>
  <li>
    <p><strong>LLMs sind keine Suchmaschinen!</strong><br />
Wenn man ein LLM etwas fragt, erhält man eine “Durchschnittsantwort” aus dem Trainingsmaterial. Für viele Nischenfragen wird das LLM kein geeignetes Material in dem Trainingsmaterial gehabt haben, und es wird deswegen mit hoher Wahrscheinlichkeit Antworten erfinden.<br />
Auch werden LLMs, da sie trainiert werden müssen, nie komplett aktuelle Informationen haben. Einige Modelle können selbstständig das Internet durchsuchen, aber auch hier legt man sein Vertrauen in das Modell, korrekte Informationen zu finden, was sie oft nicht zustande bekommen. Möglicherweise haben Sie bereits den folgenden Screenshot gesehen:</p>
    <div class="flexcontainer" style="display: flex; align-items: center;">
          <div style="flex: 1;">
              <img src="assets/pizza.png" alt="" style="max-width: 100%; height: auto;" />
          </div>
          <div class="info" style="flex: 1; text-align: left;">
              Suche: Käse hält nicht auf Pizza<br />
              <br />
              KI-Übersicht:<br />
              Der Käse kann aus verschiedenen Gründen von der Pizza rutschen, z.B. wegen zu viel Sauce, zu viel Käse oder verdickter Sauce. Hier sind einige Dinge, die Sie ausprobieren können:<br />
              <ul>
                  <li style="margin-left: 0px;">
                  Mischen Sie den Käse in die Sauce: Dadurch wird der Käse feuchter und die Sauce trockener.<br />
                  <span style="border-bottom: 2px solid red; width: fit-content;">Sie können auch etwa 1/8 Tasse ungiftigen Klebstoff in die Sauce geben, um sie klebriger zu machen.</span>
                  </li>
              </ul>
          </div>
      </div>
    <div>
  Dies ist eine echte Antwort von Google Gemini kurz nach der Veröffentlichung. Hier ist ein <a href="https://www.spiegel.de/netzwelt/web/google-lustige-suchergebnisse-ki-empfiehlt-klebstoff-zum-pizzabacken-a-a4b0ff1a-40ad-44c8-b879-f39315a865c7" target="_self">Artikel</a> zum Thema. Jemand schlug diese Lösung in einem Reddit-Post vor, und Gemini war nicht in der Lage dies als Witz zu identifizieren.<br />
  In diesem Fall ist es sehr einfach zu sehen, dass die Information falsch ist. Trotzdem schreibt es das Modell mit absoluter Überzeugung. Wäre die Unwahrheit über etwas Komplizierteres als Pizza, wird es deutlich schwieriger, diese Probleme zu entdecken, vor allem wenn man selbst wenig Ahnung vom Thema hat.<br />

</div>
  </li>
</ul>
<p>Und aus diesen drei Punkten, können wir nun sehr viele Anwendungsbereiche und “Verhaltensregeln” extrapolieren, bei denen wir die Antworten von LLMs möglichst wahrheitsvoll und relevant gehalten werden:</p>

<h2 id="kontext-ist-könig">Kontext ist König</h2>

<p>Aber LLMs, <strong>Sprach</strong>modelle, sind phänomenal darin, <strong>geschriebene Sprache</strong> umzuformulieren! Natürlich kann man ihnen viele Fragen direkt stellen und man erhält oft eine korrekte Antwort, aber indem man ihnen zuerst Kontext gibt, wie beispielsweise ein Wikipedia Artikel, stellt man dem LLM eine konkrete Basis von <strong>verifizierten</strong> Informationen zur Verfügung, welche das LLM direkt brauchen kann, und es auch in die richtige Region im früher besprochenen “Konzept-Raum” lenken.<br />
Auch sind LLMs wundervolle Diskussionspartner, welche dir helfen können, wenn Sie mit einer Idee irgendwo feststecken.</p>

<h3 id="text-analyse">Text Analyse</h3>
<p>Ein LLM ist auch dafür geeignet, Texte zu analysieren, aber man muss darauf achten, dem Modell genügend Informationen zur Verfügung zu stellen.</p>

<chat class="dont">Was ist der Symbolismus hinter XY auf Seite 254 in Blabla?</chat>

<p>Es ist möglich, dass das LLM das Buch kennt, aber das ist nicht garantiert. Auch das LLM auf eine Seite zu verweisen, funktioniert nicht, da verschiedene Editionen des Buches eventuell verschiedene Seitenzahlen haben. Diese vielen Unsicherheiten ermöglichen es dem LLM, stark zu halluzinieren.<br />
Stattdessen stellen Sie dem LLM den relevanten Textausschnitt zur Verfügung und eventuell auch noch einige Zusatzinformationen, wie z.B. der Titel des Buches und der Name des Autors, um es in die richtige Region im Konzept-Raum zu lenken.</p>

<chat>&gt; [Relevante Textstelle]
Was ist der Symbolismus hinter XY in diesem Textausschnitt aus Blabla von [Name des Autors/der Autorin]?</chat>

<h3 id="erklären-von-konzepten">Erklären von Konzepten</h3>
<p>LLMs sind ein Geschenk des Himmels, wenn es darum geht, etwas erklärt zu bekommen. Egal wie peinlich die Frage, ein LLM wird immer antworten und egal wie lange es dauert, ein LLM gibt nie auf.</p>
<chat class="dont">Wie funktioniert der Citratzyklus?</chat>
<p>Aber auch hier ist es besser, nicht zu hoffen, dass das LLM selbst korrekte Informationen aus dem Ether beschwören kann. Ausserdem ist in diesem Beispiel keinerlei Aussage gegeben, wie detailliert die Antwort sein sollte, also wird das Modell höchstwahrscheinlich mit Fachjargon daherkommen.<br />
Wenn vorhanden, ist es immer eine gute Idee, dem LLM einen kurzen Abschnitt aus Ihrem Textbuch zu geben. So erhält das LLM nicht nur korrekte Informationen, sondern Es versteht sofort, auf welchem Niveau diese Informationen präsentiert werden müssen:</p>

<chat>&gt; [Textbuch ausschnitt über den Citratzylkus]
Erkläre mir den Citratzyklus anhand dieses Textes.</chat>

<h2 id="negationen-wenn-möglich-weglassen">Negationen wenn möglich Weglassen</h2>
<p>Erinnern Sie sich daran, dass ein LLM von dem schon vorhandenen Text in eine “Richtung” im Konzept-Raum geleitet wird. Das heisst, dass Negation das LLM möglicherweise verwirren könnte und das negierte Wort trotzdem noch die Antwort beeinflusst.</p>

<chat class="dont">Gib mir ein Kuchenrezept ohne Mehl.</chat>

<chatb>Hier ist ein einfaches Rezept für einen leckeren Kuchen ohne Mehl – ein Schokoladen-Nuss-Kuchen:
<b>Zutaten:</b><ul>
    <li>200 g Zartbitterschokolade</li>
    <li>150 g gemahlene Mandeln oder Haselnüsse (alternativ: andere Nüsse nach Wahl)</li>
    <li>150 g Zucker</li>
    <li>5 Eier</li>
    <li style="border-bottom: 2px solid red; width: fit-content;">250 g Mehl</li>
    <li>150 g Butter</li>
    <li>1 TL Vanilleextrakt</li>
    <li>1 Prise Salz</li>
    <li>Optional: Puderzucker zum Bestreuen</li>
</ul>
<b>Zubereitung:</b>
[...]
</chatb>

<p>Dies hat sich seit der Einführung von ChatGPT stark verbessert und heute ist es nur noch ein grösseres Problem bei dem Generieren von Bildern (fragten Sie ChatGPT mal ein Bild ohne eine roten Ballon zu generieren). Es kann dennoch von Vorteil sein, Verneinungen zu vermeiden:</p>
<chat>Gib mir ein Rezept für einen glutenfreien Kuchen.</chat>
<p>Oder wenn die Verneinung wirklich nötig ist, aber das LLM verwirrt ist, kann es hilfreich sein, die Verneinung zu betonen:</p>
<chat>Gib mir ein Kuchenrezept **OHNE** Mehl.</chat>

<h2 id="positives-bias">Positives Bias</h2>

<p>Kommerzielle LLMs wie ChatGPT haben ein starkes positives Bias (weil sie mit hauptsächlich positiven Interaktionen trainiert wurden). Wenn man ein LLM fragt</p>

<chat class="dont">Ist dieser Text gut geschrieben?
&gt; [Ihr Text hier]</chat>
<p>Wird es in aller Wahrscheinlichkeit sehr positiv Antworten und fast kein brauchbares Feedback liefern.
Das heisst, Wenn man tatsächlich ehrliches Feedback wünscht, sollte man dem LLM sehr direkt mitteilen, dass es weniger positiv sein kann:</p>

<chat>Korrigiere den nachfolgenden Text und lass mich wissen, wenn etwas nicht gut verständlich ist. Bitte sei sehr kritisch.
&gt; [Ihr Text hier]</chat>

<p>Was auch helfen kann, ist das LLM die Pros und Kons von Dingen auflisten zu lassen:</p>

<chat>Liste die Stärken und Schwächen dieses Texts:
&gt; [Ihr Text hier]</chat>

<h2 id="llms-können-sehr-gut-programmieren">LLMs können sehr gut programmieren</h2>
<p>Solange nichts zu Komplexes verlangt wird, können LLMs fast alles programmieren. Unangenehm wird es, wenn das LLM einen Fehler macht, und weder der Nutzer noch das LLM ihn finden können.<br />
Also lohnt es sich auch im Zeitalter von ChatGPT trotzdem noch selbst Programmieren zu lernen, wenn man Interesse daran hat. Übrigens sind auch zu diesem Zweck LLMs sehr nützlich, da sie oft Code den man selbst nicht versteht, erklären können:</p>

<chat>Was macht dieser Code?
<tt style="font-size: 0.7em; white-space: pre-wrap;">let checks = document.querySelectorAll("#plotlyContainer input[type='checkbox']")
let active = []
checks.forEach((check) =&gt; {
  if (check.checked) {
    active.push(check.id)
  }
});
if (active.length == 0) {
  layout = layout0D
  trace = {
    x: generateRandomArray(words.length, -2, 2, 0.225),
    y: generateRandomArray(words.length, -2, 2, 0.225),
    mode: 'text',
    text: words,
    textposition: 'middle center',
    type: 'scatter',
    name: 'none'
  };
}</tt></chat>
<chatb>Der Code prüft, welche Checkboxen im Element <code>#plotlyContainer</code> aktiviert sind, speichert deren IDs, und erstellt einen Plotly-Trace. Wenn keine Checkbox aktiv ist, wird ein 0D-Layout geladen und ein Scatterplot mit zufällig generierten x- und y-Werten sowie Text (aus <code>words</code>) erzeugt.</chatb>

<p>Aber natürlich beherrschen LLMs nicht jede Programmiersprache, sondern nur diese, die in ihrem Trainingsmaterial genügend vorgekommen sind. Deswegen sind sie oft äusserst clever wenn es zu Python und JavaScript kommt, aber je obskurer die Sprache wird, desto schlechter die Qualität des generierten Codes. Auch kennt ein LLM nicht jedes Paket. Wenn sie mit etwas nicht vertraut sind, neigen sie dazu, Funktionen zu erfinden, welche die perfekte Lösung für ein Problem wären, aber leider nicht existieren.</p>

<h2 id="llms-sind-starke-übersetzer">LLMs sind starke Übersetzer</h2>
<p>Vor allem in der gleichen Sprachfamilie sind LLMs zuverlässige Übersetzer. <tooltip>(Können Sie sich vorstellen, warum?)<span>Das gleiche Wort in verschiedenen Sprachen werden dank der gleichen Bedeutung im Konzept-Raum sehr nahe beieinander liegen, also muss das Modell essenziell nur die Grammatik anpassen.</span></tooltip><br />
Auch beim Übersetzen zwischen Sprachfamilien sind, dank ihrem Kontext-Verständnisses, LLMs eine der zuverlässigsten automatisierten Methoden, die uns zur Verfügung stehen. Jedoch wird die Übersetzung definitiv Nuancen verlieren und oft sogar Fehler enthalten. Vor allem, wenn eine der Sprachen nicht prominent im Trainingsmaterial des LLMs vertreten war. Heute sind LLMs also noch kein Ersatz dazu, eine Sprache zu lernen.</p>

<h2 id="llms-wurden-nicht-dazu-gemacht-zu-rechnen">LLMs wurden NICHT dazu gemacht zu rechnen</h2>
<p>LLMs sind <u>Sprach</u>modelle, und können Sprache “Verstehen”. In ihrem extrem umfangreichen Trainingsmaterial finden sich zwar zahlreiche mathematische Inhalte und Beispiele, aber LLMs sind nicht primär für die Durchführung exakter Berechnungen ausgelegt. Sie basieren auf Wahrscheinlichkeiten und Mustern, die sie aus ihrem Training ableiten, anstatt auf präzis definierter mathematischen Logik wie spezialisierte Software.</p>

<p>Das bedeutet, dass sie bei einfachen Rechnungen oft richtige Antworten liefern können, aber bei komplexeren oder ungewöhnlichen Berechnungen Fehler machen. Besonders problematisch wird es, wenn die Antwort oberflächlich korrekt erscheint, aber tatsächlich falsch ist, da das Modell keine Möglichkeit hat, seine Antworten zu überprüfen.</p>

<p>Wenn es um Rechnen geht, verlassen Sie sich lieber auf Ihren Taschenrechner, oder versuchen Sie ihr Glück mit <a href="https://www.wolframalpha.com/">WolframAlpha</a>.</p>

<h1 id="abschluss">Abschluss</h1>
<p>Wie sich das Feld der Sprachverarbeitung in den kommenden Jahren entwickeln wird, bleibt angesichts des rasanten technischen Fortschritts schwer vorherzusagen. Das grosse Interesse an KI und die Geheimhaltung führender Unternehmen wie OpenAI und Anthropic erschweren es Aussenstehenden die Entwicklungen vollständig zu überblicken. Doch eines der grössten Probleme zeichnet sich bereits ab: Der Mangel an neuen Trainingsdaten. Da immer mehr Inhalte im Internet von KI generiert werden, könnten Modelle in Zukunft an Qualität verlieren, wenn sie auf solchen Daten basieren. Dies wirft die Frage auf, ob neue Methoden entwickelt werden können, um effizienter mit vorhandenen Daten zu arbeiten, oder ob gänzlich neue Ansätze erforderlich sind.</p>

<p>Neben den High-End-Modellen, die auf grossen Serverfarmen laufen, gewinnen auch lokale Modelle zunehmend an Bedeutung. Open-Source-Modelle wie Metas <a href="https://www.llama.com/llama3_1/">Llama 3.1 8b</a> zeigen, dass leistungsstarke KI-Systeme auch auf moderner Konsumer-Hardware betrieben werden können. Solche Modelle könnten in Zukunft eine grössere Rolle in der täglichen Anwendung spielen, beispielsweise bei der Verarbeitung privater Daten direkt auf dem Gerät, ohne dass je Informationen ins Internet gesendet werden müssen.</p>

<p>Auch grosse Unternehmen wie Microsoft und Apple erkennen das Potenzial lokaler LLMs. Mit Tools wie Microsofts <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">Copilot+</a> und Apples <a href="https://www.apple.com/apple-intelligence/">Apple Intelligence</a> werden Systeme eingeführt, die ohne Internetverbindung funktionieren und so mehr Privatsphäre gewährleisten. Wahrscheinlich werden lokale Modelle nie ganz so leistungsstark sein können wie die Cloud-Lösungen, aber trotz der extrem limitierten Rechenkraft von Konsumer-Hardware steigert sich ihre Leistungsfähigkeit konstant.<br />
Es ist gut möglich, dass jeder von uns in der nahen Zukunft ein kleines, komplett eigenständiges ChatGPT in der Hosentasche haben wird.</p>

<h1 id="quiz">Quiz</h1>

<style>
    body {
        font-family: Arial, sans-serif;
        margin: 20px;
    }
    .question {
        margin-bottom: 20px;
        padding: 10px;
        border-radius: 5px;
        border: none;
        transition: background-color .5s ease,border .5s ease;
    }
    .correct {
        background-color: #d4edda;
        border: 2px solid #28a745;
    }
    .incorrect {
        background-color: #f8d7da;
        border: 2px solid #dc3545;
    }
    .unanswered {
        background-color: #f8f3d7;
        border: 2px solid #dcd935;
    }
    .explanation {
        color: blue;
        font-style: italic;
        margin-top: 5px;
    }
    .results {
        margin-top: 20px;
        font-weight: bold;
    }
</style>

<form id="quizForm">
    <div id="quizContainer"></div>
    <button type="button" onclick="submitQuiz()">Korrigieren</button>
</form>
<div id="results" class="results"></div>

<script>
    const quizData = [
        {
            question: "Wofür werden Large Language Models (LLMs) primär verwendet?",
            options: [
                { 
                    text: "Für die Durchführung komplexer Berechnungen", 
                    correct: false, 
                    explanation: "LLMs basieren auf Sprachverarbeitung und sind keine spezialisierten Werkzeuge für mathematische oder wissenschaftliche Berechnungen. Sie verwenden Wahrscheinlichkeiten, was sie bei solchen Aufgaben unzuverlässig macht." 
                },
                { 
                    text: "Zur Analyse und Generierung von natürlicher Sprache", 
                    correct: true, 
                    explanation: "Korrekt! LLMs wurden entwickelt, um Sprache zu verstehen, zu analysieren und zu generieren, was sie bei Aufgaben wie Zusammenfassung, Textübersetzung und Textgenerierung nützlich macht." 
                },
                { 
                    text: "Zur direkten Suche nach aktuellen Informationen im Internet", 
                    correct: false, 
                    explanation: "LLMs haben keinen direkten Zugriff auf aktuelle Internetdaten, es sei denn, sie sind speziell mit solchen Funktionen ausgestattet. Ihre Antworten basieren ausschliesslich auf Trainingsdaten, die möglicherweise veraltet sind." 
                },
                { 
                    text: "Für die Simulation physikalischer Experimente", 
                    correct: false, 
                    explanation: "Die Stärke von LLMs liegt in der Verarbeitung von Sprache. Sie sind nicht dafür ausgelegt, physikalische Experimente zu simulieren oder naturwissenschaftliche Probleme zu lösen." 
                }
            ]
        },
        {
            question: "Was beschreibt der \"Konzept-Raum\", den ein LLM verwendet?",
            options: [
                { 
                    text: "Den physikalischen Speicherplatz, der für das Training benötigt wird", 
                    correct: false, 
                    explanation: "Der Konzept-Raum hat nichts mit Speicherplatz oder physikalischer Hardware zu tun. Er bezieht sich auf mathematische Strukturen, die die Beziehungen zwischen Wörtern darstellen." 
                },
                { 
                    text: "Die Art, wie LLMs Texte in Tokens unterteilen", 
                    correct: false, 
                    explanation: "Die Tokenisierung ist ein separater Prozess, bei dem Texte in kleinere Einheiten zerlegt werden. Der Konzept-Raum hingegen ordnet Bedeutungen und Beziehungen zwischen diesen Einheiten mathematisch an." 
                },
                { 
                    text: "Die Anordnung von Bedeutungen und Beziehungen zwischen Wörtern", 
                    correct: true, 
                    explanation: "Korrekt! Der Konzept-Raum repräsentiert die Bedeutungen von Wörtern und ihre Beziehungen zueinander in einem hochdimensionalen mathematischen Raum." 
                },
                { 
                    text: "Die Hardware, die für die Berechnungen notwendig ist", 
                    correct: false, 
                    explanation: "Der Konzept-Raum ist eine mathematische Abstraktion und hat nichts mit der zugrunde liegenden Hardware zu tun, auf der das Modell ausgeführt wird." 
                }
            ]
        },
        {
            question: "Warum sollten LLMs nicht für Berechnungen verwendet werden?",
            options: [
                { 
                    text: "Sie haben keinen Zugriff auf Taschenrechner", 
                    correct: false, 
                    explanation: "Der Grund liegt nicht im Zugriff auf Taschenrechner, sondern in der Art und Weise, wie LLMs Sprache verstehen und Muster erkennen, anstatt mathematisch Präzise zu sein." 
                },
                { 
                    text: "Sie beruhen auf Wahrscheinlichkeiten und Mustern", 
                    correct: true, 
                    explanation: "Korrekt! LLMs sind darauf ausgelegt, Muster in Texten zu erkennen, und können daher Berechnungen nur simulieren, aber nicht mit voller Präzision ausführen." 
                },
                { 
                    text: "Sie können nur einfache Addition und Subtraktion durchführen", 
                    correct: false, 
                    explanation: "LLMs können sogar komplexere Berechnungen durchführen, aber ihre Ergebnisse sind oft unzuverlässig, weil sie keine echte mathematische Logik anwenden." 
                },
                { 
                    text: "Sie wurden ausschliesslich mit Textdaten trainiert", 
                    correct: false, 
                    explanation: "Obwohl das Trainingsmaterial grösstenteils aus Text besteht, ist dies nicht der Hauptgrund. Vielmehr fehlt ihnen die Architektur für exakte Berechnungen." 
                }
            ]
        },
        {
            question: "Warum arbeiten LLMs mit Tokens und nicht direkt mit Wörtern?",
            options: [
                { 
                    text: "Tokens vereinfachen die Verarbeitung durch kürzere Sequenzen", 
                    correct: true, 
                    explanation: "Korrekt! Durch die Verwendung von Tokens werden längere Texte effizienter verarbeitet, da Tokens eine flexiblere Einheit sind als ganze Wörter oder Buchstaben." 
                },
                { 
                    text: "Tokens tragen mehr Informationen als Wörter", 
                    correct: false, 
                    explanation: "Tokens tragen nicht per se mehr Informationen, sondern zerlegen Wörter in kleinere Einheiten, um sie leichter verarbeitbar zu machen." 
                },
                { 
                    text: "Tokens sind mathematisch schwieriger zu modellieren", 
                    correct: false, 
                    explanation: "Tokens erleichtern tatsächlich die mathematische Modellierung, indem sie die Komplexität reduzieren und konsistentere Einheiten schaffen." 
                },
                { 
                    text: "Tokens ermöglichen eine präzisere Übersetzung zwischen Sprachen", 
                    correct: false, 
                    explanation: "Tokens erleichtern die Verarbeitung von Sprache, aber die Genauigkeit der Übersetzung hängt von vielen anderen Faktoren ab." 
                }
            ]
        },
        {
            question: "Wie sollten Schüler mit LLMs arbeiten, um zuverlässige Ergebnisse zu erhalten?",
            options: [
                { 
                    text: "Nur allgemeine Fragen stellen", 
                    correct: false, 
                    explanation: "Allgemeine Fragen ohne zusätzlichen Kontext führen oft zu oberflächlichen oder ungenauen Antworten, da das LLM keine klaren Anweisungen hat." 
                },
                { 
                    text: "Präzise Eingabe mit zusätzlichen Informationen liefern", 
                    correct: true, 
                    explanation: "Korrekt! Je präziser die Eingabe, desto höher ist die Chance, dass das LLM eine relevante und korrekte Antwort liefert." 
                },
                { 
                    text: "Das LLM für alle Aufgaben ohne Überprüfung der Antworten nutzen", 
                    correct: false, 
                    explanation: "Die Antworten eines LLMs sollten immer überprüft werden, da sie Fehler enthalten oder sogar erfunden sein können (Halluzinationen)." 
                },
                { 
                    text: "LLMs niemals in schulischen Kontexten einsetzen", 
                    correct: false, 
                    explanation: "LLMs können sinnvoll eingesetzt werden, z.B. zur Erklärung von Konzepten oder zur Strukturierung von Ideen, wenn sie korrekt genutzt werden." 
                }
            ]
        }
    ];


    function renderQuiz() {
        const quizContainer = document.getElementById('quizContainer');
        quizData.forEach((questionData, index) => {
            const questionDiv = document.createElement('div');
            questionDiv.classList.add('question');
            questionDiv.setAttribute('id', `question${index}`);
            questionDiv.innerHTML = `
                <p>${index + 1}. ${questionData.question}</p>
                ${questionData.options.map((option, optIndex) => `
                    <label>
                        <input type="radio" name="question${index}" value="${optIndex}">
                        ${option.text}
                    </label><br>
                `).join('')}
                <div id="explanation${index}" class="explanation"></div>
            `;
            quizContainer.appendChild(questionDiv);
        });
    }

    function submitQuiz() {
        let correctCount = 0;
        quizData.forEach((questionData, index) => {
            const selectedOption = document.querySelector(`input[name="question${index}"]:checked`);
            const explanationDiv = document.getElementById(`explanation${index}`);
            const questionDiv = document.getElementById(`question${index}`);
            explanationDiv.textContent = "";
            questionDiv.classList.remove('correct', 'incorrect', 'unanswered');

            if (selectedOption) {
                const selectedValue = parseInt(selectedOption.value);
                const option = questionData.options[selectedValue];
                explanationDiv.textContent = option.explanation;

                if (option.correct) {
                    correctCount++;
                    questionDiv.classList.add('correct');
                } else {
                    questionDiv.classList.add('incorrect');
                }
            } else {
                explanationDiv.textContent = "Keine Antwort Ausgewählt.";
                questionDiv.classList.add('unanswered');
            }
        });

        const resultsDiv = document.getElementById('results');
        resultsDiv.textContent = `Sie haben ${correctCount} von ${quizData.length} Fragen korrekt beantwortet!`;
    }

    renderQuiz();
</script>


        <script src="assets/plot.js"></script>

    </main>
    <!-- <div class="wrapper">
    <h2 class="footer-heading">KI - Künstliche Intelligenz und wie man sie braucht</h2>
    <div class="footer-col-wrapper">
        <div class="footer-col footer-col-1">
        <ul class="contact-list">
            <li class="p-name">KI - Künstliche Intelligenz und wie man sie braucht</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
        </div>
    </div>

        <div class="footer-col footer-col-3">
        <p></p>
        </div>
    </div>

    </div> -->




</body><style id="stylus-3" type="text/css" class="stylus">* {
    scrollbar-width: thin;
}
</style><style id="stylus-8" type="text/css" class="stylus">

.ytp-pause-overlay.ytp-scroll-min {display: none;}</style></html>